{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b552b5f0",
   "metadata": {},
   "source": [
    "# Deep Research Framework with Agentic RAG\n",
    "\n",
    "This notebook implements a deep research framework that uses the Agentic RAG system for each sub-query. The architecture follows your correct approach:\n",
    "\n",
    "1. **Search Planner**: Generate multiple focused queries from the original complex query\n",
    "2. **Agentic RAG Processing**: For each sub-query, use the full Agentic RAG framework with all reliability features\n",
    "3. **Answer Aggregation**: Synthesize all individual answers into a comprehensive final report\n",
    "\n",
    "The system combines the reliability of Agentic RAG with the comprehensive coverage of deep research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "system_architecture",
   "metadata": {},
   "source": [
    "## System Architecture\n",
    "\n",
    "```\n",
    "Main Deep Research Framework\n",
    "│\n",
    "├── Search Planner (generate multiple queries)\n",
    "│\n",
    "├── For each query:\n",
    "│   ├── AgenticRAG Framework\n",
    "│   │   ├── Query Routing\n",
    "│   │   ├── RAG Pipeline (with all reliability features)\n",
    "│   │   │   ├── Document Retrieval\n",
    "│   │   │   ├── Document Grading\n",
    "│   │   │   ├── Query Rewriting (if needed)\n",
    "│   │   │   └── Web Search Fallback\n",
    "│   │   └── Answer Generation\n",
    "│   └── Store individual answer\n",
    "│\n",
    "└── Final Report Generator (aggregate all answers)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environment_setup",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e27eb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "import requests\n",
    "import os\n",
    "from typing import Annotated, TypedDict, List, Dict\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langgraph.graph import END, StateGraph\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Note: Using local Ollama Qwen model for LLM\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "research_state",
   "metadata": {},
   "source": [
    "### 1.2 Research State Definition\n",
    "\n",
    "Define the ResearchState for the main deep research workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80a1982",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], operator.add]\n",
    "    original_query: str\n",
    "    search_queries: List[str]\n",
    "    query_answers: Dict[str, str]  # query -> answer\n",
    "    current_query_index: int\n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowledge_base",
   "metadata": {},
   "source": [
    "## 2. Knowledge Base\n",
    "\n",
    "### 2.1 Vector Store Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0844fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yzhou62/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load pre-built FAISS database containing LLM-related content\n",
    "db_path = './db/faiss_ilianweng_db'\n",
    "vector_store = FAISS.load_local(\n",
    "    folder_path=db_path,\n",
    "    embeddings=HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\"),\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Configure retriever to return top 2 documents for efficient retrieval\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools_integrations",
   "metadata": {},
   "source": [
    "## 3. Tools and Integrations\n",
    "\n",
    "### 3.1 Tool Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b47c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for web search queries using Pydantic for validation\n",
    "class SearchQuery(BaseModel):\n",
    "    query: str = Field(description=\"Search query for web search\")\n",
    "\n",
    "\n",
    "# Web search tool using Serper API for real-time information retrieval\n",
    "@tool(args_schema=SearchQuery)\n",
    "def web_search_tool(query: str):\n",
    "    \"\"\"Get real-time Internet information using Serper API\"\"\"\n",
    "    # Get API key from environment variable or use placeholder\n",
    "    api_key = os.environ.get('SERPER_API_KEY', 'd6205f8378e105dc9afdcb2dbbd44521c716b9c4')\n",
    "    \n",
    "    url = \"https://google.serper.dev/search\"\n",
    "    payload = json.dumps({\n",
    "        \"q\": query,\n",
    "        \"num\": 3,  # Retrieve top 3 search results for comprehensive coverage\n",
    "    })\n",
    "    headers = {\n",
    "        'X-API-KEY': api_key,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=payload, timeout=10)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'organic' in data:\n",
    "            results = []\n",
    "            for item in data['organic'][:3]:  # Take top 3 results\n",
    "                result = {\n",
    "                    'title': item.get('title', ''),\n",
    "                    'snippet': item.get('snippet', ''),\n",
    "                    'link': item.get('link', '')\n",
    "                }\n",
    "                results.append(result)\n",
    "            return json.dumps(results, ensure_ascii=False)\n",
    "        else:\n",
    "            return json.dumps({\"error\": \"No organic results found\"}, ensure_ascii=False)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return json.dumps({\"error\": f\"Network error occurred: {str(e)}\"}, ensure_ascii=False)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return json.dumps({\"error\": f\"Failed to parse response: {str(e)}\"}, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"An unexpected error occurred: {str(e)}\"}, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# Define schema for RAG queries using Pydantic for validation\n",
    "class RagQuery(BaseModel):\n",
    "    query: str = Field(description=\"Query for RAG document retrieval\")\n",
    "\n",
    "\n",
    "# RAG retrieval tool for internal document search from vector store\n",
    "@tool(args_schema=RagQuery)\n",
    "def rag_retrieval_tool(query: str):\n",
    "    \"\"\"\n",
    "    Retrieves documents from the RAG vector store.\n",
    "    This tool is the core of the Retrieval-Augmented Generation pipeline.\n",
    "    \"\"\"\n",
    "    # 1. Retrieve relevant documents using the configured FAISS retriever\n",
    "    documents = retriever.invoke(query)\n",
    "        \n",
    "    # 2. Format retrieved documents as a single context string\n",
    "    doc_contents = \"\\n\\n\".join(d.page_content for d in documents)\n",
    "    return json.dumps({\"documents\": doc_contents})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm_configuration",
   "metadata": {},
   "source": [
    "### 3.2 LLM Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0268aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LLM with local Qwen model via Ollama for privacy and cost efficiency\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"qwen3:8b\",\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    openai_api_key=\"<KEY>\",  # Placeholder key (not used for local models)\n",
    "    temperature=0.3  # Low temperature for consistent, factual responses\n",
    ")\n",
    "\n",
    "# Bind tools for structured tool calling in LangGraph workflow\n",
    "llm_with_tools = llm.bind_tools([rag_retrieval_tool, web_search_tool])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agentic_rag_components",
   "metadata": {},
   "source": [
    "## 4. Agentic RAG Components\n",
    "\n",
    "### 4.1 Agentic RAG State Definition\n",
    "\n",
    "Define the AgentState for the Agentic RAG workflow that will be used for each sub-query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "agent_state",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], operator.add]\n",
    "    rag_retries: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agentic_rag_nodes",
   "metadata": {},
   "source": [
    "### 4.2 Agentic RAG Node Functions\n",
    "\n",
    "Define the core node functions that implement the Agentic RAG workflow for each sub-query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a57dfb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route query to direct response, RAG, or web search\n",
    "def route_query(state: AgentState):\n",
    "    \"\"\"\n",
    "    Routes the user's question to the most appropriate processing path:\n",
    "    - Direct response (for greetings or general knowledge)\n",
    "    - RAG pipeline (for LLM-specific topics)\n",
    "    - Web search (for time-sensitive information)\n",
    "    \"\"\"\n",
    "    print(\"---ROUTING QUERY---\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # System prompt for query routing decisions\n",
    "    system_prompt = \"\"\"You are a query router. If the question is a simple greeting like 'hello' or can be answered directly without any retrieval or external information, respond with a friendly, concise answer. Otherwise, determine if the question can be answered using internal knowledge (call rag_retrieval_tool with the query) or requires external real-time information (call web_search_tool with the query). Use the vectorstore for questions on LLM agents, prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web-search. Do not provide any explanation or reasoning beyond the direct answer if applicable.\"\"\"\n",
    "    \n",
    "    response = llm_with_tools.invoke([SystemMessage(content=system_prompt), last_message])\n",
    "    print(f\"Routing decision: {response.tool_calls if hasattr(response, 'tool_calls') else 'Direct response'}\")\n",
    "    \n",
    "    tool_calls = response.tool_calls if hasattr(response, 'tool_calls') else []\n",
    "    if tool_calls:\n",
    "        # Return tool call for RAG or web search processing\n",
    "        return {\"messages\": [AIMessage(content=\"\", tool_calls=tool_calls)], \"last_query\": tool_calls[0][\"args\"][\"query\"]}\n",
    "    else:\n",
    "        # Return direct response for simple queries\n",
    "        return {\"messages\": [AIMessage(content=response.content)]}\n",
    "\n",
    "# RAG retrieval node - fetch documents from vector store\n",
    "def rag_retrieval(state: AgentState):\n",
    "    \"\"\"\n",
    "    Executes the RAG retrieval tool to fetch relevant documents from the vector store.\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVING DOCUMENTS FROM RAG---\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    query = last_message.tool_calls[0][\"args\"][\"query\"]\n",
    "    \n",
    "    # Invoke the RAG retrieval tool with the query\n",
    "    result = rag_retrieval_tool.invoke({\"query\": query})\n",
    "    doc_content = json.loads(result)[\"documents\"]\n",
    "    \n",
    "    # Return retrieved documents as a tool message\n",
    "    return {\"messages\": [ToolMessage(content=doc_content, tool_call_id=last_message.tool_calls[0][\"id\"])]}\n",
    "\n",
    "# Grade RAG documents for relevance\n",
    "def grade_rag_documents(state: AgentState):\n",
    "    \"\"\"\n",
    "    Grades the relevance of retrieved documents to determine if they can answer the user's question.\n",
    "    Returns 'Documents relevant' or 'Documents not relevant' based on LLM assessment.\n",
    "    \"\"\"\n",
    "    print(\"---GRADING RAG DOCUMENTS---\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    question = state[\"messages\"][0].content\n",
    "    \n",
    "    # Handle case where no documents were retrieved\n",
    "    if not last_message.content:\n",
    "        print(\"No documents retrieved, grading as irrelevant.\")\n",
    "        return {\"messages\": [AIMessage(content=\"Documents not relevant.\")], \"rag_retries\": state.get(\"rag_retries\", 0) + 1}\n",
    "\n",
    "    # Prompt LLM to grade document relevance\n",
    "    grading_prompt = f\"\"\"You are a grader assessing the relevance of retrieved documents to a user question.\n",
    "    User question: {question}\n",
    "    Retrieved documents:\n",
    "    ---\n",
    "    {last_message.content}\n",
    "    ---\n",
    "    Are the documents relevant to the question? Respond with 'yes' or 'no' only, nothing else.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=grading_prompt)])\n",
    "    is_relevant = 'yes' in response.content.lower()\n",
    "    print(f\"Grading decision: {is_relevant}\")\n",
    "    \n",
    "    if is_relevant:\n",
    "        # Documents are relevant, reset retry counter and proceed to generation\n",
    "        return {\"messages\": [AIMessage(content=\"Documents relevant.\")], \"rag_retries\": 0}\n",
    "    else:\n",
    "        # Documents are not relevant, increment retry counter\n",
    "        return {\"messages\": [AIMessage(content=\"Documents not relevant.\")], \"rag_retries\": state.get(\"rag_retries\", 0) + 1}\n",
    "\n",
    "# Rewrite RAG query when initial retrieval fails\n",
    "def rewrite_query(state: AgentState):\n",
    "    \"\"\"\n",
    "    Rewrites the query to improve retrieval performance when the first attempt failed.\n",
    "    \"\"\"\n",
    "    print(\"---REWRITING RAG QUERY---\")\n",
    "    question = state[\"messages\"][0].content\n",
    "    \n",
    "    # Prompt LLM to generate a better query\n",
    "    rewrite_prompt = f\"\"\"You are a query rewriting expert. The user's original question is: \"{question}\".\n",
    "    This query will be used to retrieve documents from a vector database. Please generate a better, more specific query.\n",
    "    Do not think step-by-step or add any explanations. Respond with the rewritten query only, nothing else.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=rewrite_prompt)])\n",
    "    rewritten_query = response.content.strip()\n",
    "    \n",
    "    # Clean up potential markdown code block artifacts\n",
    "    if '```' in rewritten_query:\n",
    "        rewritten_query = rewritten_query.split('```', 1)[-1].strip()\n",
    "    \n",
    "    if rewritten_query:\n",
    "        print(f\"Rewritten query: {rewritten_query}\")\n",
    "        tool_call = {\"name\": \"rag_retrieval_tool\", \"args\": {\"query\": rewritten_query}, \"id\": \"call_\" + str(hash(rewritten_query))[:8], \"type\": \"tool_call\"}\n",
    "        return {\"messages\": [AIMessage(content=\"\", tool_calls=[tool_call])]}\n",
    "    else:\n",
    "        print(\"No rewritten query generated, using original question.\")\n",
    "        tool_call = {\"name\": \"rag_retrieval_tool\", \"args\": {\"query\": question}, \"id\": \"call_\" + str(hash(question))[:8], \"type\": \"tool_call\"}\n",
    "        return {\"messages\": [AIMessage(content=\"\", tool_calls=[tool_call])]}\n",
    "\n",
    "# Fallback to web search after max RAG retries\n",
    "def fallback_to_web_search(state: AgentState):\n",
    "    \"\"\"\n",
    "    Prepares a tool call for web search when RAG approaches have failed after maximum retries.\n",
    "    \"\"\"\n",
    "    print(\"---FALLING BACK TO WEB SEARCH---\")\n",
    "    query = state[\"messages\"][0].content\n",
    "    \n",
    "    tool_call = {\"name\": \"web_search_tool\", \"args\": {\"query\": query}, \"id\": \"call_\" + str(hash(query))[:8], \"type\": \"tool_call\"}\n",
    "    return {\"messages\": [AIMessage(content=\"\", tool_calls=[tool_call])]}\n",
    "\n",
    "# Execute web search using Serper API\n",
    "def web_search(state: AgentState):\n",
    "    \"\"\"\n",
    "    Performs a web search using the Serper API to get real-time information.\n",
    "    \"\"\"\n",
    "    print(\"---PERFORMING WEB SEARCH---\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    query = last_message.tool_calls[0][\"args\"][\"query\"]\n",
    "    \n",
    "    search_results = web_search_tool.invoke({\"query\": query})\n",
    "    return {\"messages\": [ToolMessage(content=search_results, tool_call_id=last_message.tool_calls[0][\"id\"])]}\n",
    "\n",
    "# Generate final answer based on context\n",
    "def generate_answer(state: AgentState):\n",
    "    \"\"\"\n",
    "    Generates a final answer based on the retrieved context (from RAG or web search).\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING FINAL ANSWER---\")\n",
    "    question = state[\"messages\"][0].content\n",
    "    \n",
    "    # Find the actual context (from RAG retrieval or web search)\n",
    "    context = \"\"\n",
    "    messages = state[\"messages\"]\n",
    "    for i in range(len(messages) - 1, -1, -1):\n",
    "        if isinstance(messages[i], ToolMessage):\n",
    "            context = messages[i].content\n",
    "            break\n",
    "        # If we find the \"Documents relevant\" message, the context is in the previous ToolMessage\n",
    "        elif isinstance(messages[i], AIMessage) and \"Documents relevant\" in messages[i].content:\n",
    "            # Look for the ToolMessage before this grading message\n",
    "            for j in range(i - 1, -1, -1):\n",
    "                if isinstance(messages[j], ToolMessage):\n",
    "                    context = messages[j].content\n",
    "                    break\n",
    "            break\n",
    "    \n",
    "    # Prompt LLM to generate answer using context\n",
    "    generation_prompt = f\"\"\"You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
    "    If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "    Question: {question}\n",
    "    Context:\n",
    "    {context}\n",
    "    ---\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=generation_prompt)])\n",
    "    return {\"messages\": [AIMessage(content=response.content)]}\n",
    "\n",
    "# Define conditional routing logic for Agentic RAG\n",
    "def get_route(state: AgentState):\n",
    "    \"\"\"\n",
    "    Determines the next node based on the routing decision.\n",
    "    \"\"\"\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    if not last_msg.tool_calls:\n",
    "        return \"__end__\"\n",
    "    tool_name = last_msg.tool_calls[0][\"name\"]\n",
    "    if tool_name == \"rag_retrieval_tool\":\n",
    "        return \"rag_retrieval\"\n",
    "    elif tool_name == \"web_search_tool\":\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        return \"__end__\"\n",
    "\n",
    "# Adaptive RAG routing logic\n",
    "def should_retry_rag(state: AgentState):\n",
    "    \"\"\"\n",
    "    Implements the Adaptive RAG decision logic:\n",
    "    - If documents are relevant → generate answer\n",
    "    - If retries < 3 → rewrite query\n",
    "    - If retries >= 3 → fallback to web search\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"Documents relevant.\" in last_message.content:\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        if state.get(\"rag_retries\", 0) >= 3:\n",
    "            return \"fallback_to_web_search\"\n",
    "        else:\n",
    "            return \"rewrite\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agentic_rag_graph",
   "metadata": {},
   "source": [
    "### 4.3 Agentic RAG Graph Construction\n",
    "\n",
    "Build the LangGraph workflow for the Agentic RAG that will be used for each sub-query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d8afbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LangGraph workflow for Agentic RAG\n",
    "rag_workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add all nodes to the workflow\n",
    "rag_workflow.add_node(\"route_query\", route_query)\n",
    "rag_workflow.add_node(\"rag_retrieval\", rag_retrieval)\n",
    "rag_workflow.add_node(\"grade_rag_documents\", grade_rag_documents)\n",
    "rag_workflow.add_node(\"rewrite_query\", rewrite_query)\n",
    "rag_workflow.add_node(\"fallback_to_web_search\", fallback_to_web_search)\n",
    "rag_workflow.add_node(\"web_search\", web_search)\n",
    "rag_workflow.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "# Set entry point for the workflow\n",
    "rag_workflow.set_entry_point(\"route_query\")\n",
    "\n",
    "# Add edges and conditional routing\n",
    "rag_workflow.add_conditional_edges(\n",
    "    \"route_query\",\n",
    "    get_route,\n",
    "    {\n",
    "        \"rag_retrieval\": \"rag_retrieval\",\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_workflow.add_edge(\"rag_retrieval\", \"grade_rag_documents\")\n",
    "rag_workflow.add_conditional_edges(\n",
    "    \"grade_rag_documents\",\n",
    "    should_retry_rag,\n",
    "    {\n",
    "        \"generate\": \"generate_answer\",\n",
    "        \"rewrite\": \"rewrite_query\",\n",
    "        \"fallback_to_web_search\": \"fallback_to_web_search\",\n",
    "    }\n",
    ")\n",
    "rag_workflow.add_edge(\"rewrite_query\", \"rag_retrieval\")\n",
    "rag_workflow.add_edge(\"fallback_to_web_search\", \"web_search\")\n",
    "rag_workflow.add_edge(\"web_search\", \"generate_answer\")\n",
    "rag_workflow.add_edge(\"generate_answer\", END)\n",
    "\n",
    "# Compile the graph into an executable workflow\n",
    "rag_graph = rag_workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deep_research_components",
   "metadata": {},
   "source": [
    "## 5. Deep Research Components\n",
    "\n",
    "### 5.1 Deep Research Node Functions\n",
    "\n",
    "Define the core node functions for the main deep research workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deep_research_nodes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Planner: Generate multiple search queries from the original query\n",
    "def search_planner(state: ResearchState):\n",
    "    \"\"\"\n",
    "    Generates multiple search queries based on the original query using the LLM.\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING SEARCH QUERIES---\")\n",
    "    original_query = state[\"original_query\"]\n",
    "    \n",
    "    planner_prompt = f\"\"\"You are a research query planner. Generate 3-5 diverse search queries that would help research the topic: \"{original_query}\".\n",
    "    Make the queries specific and focused on different aspects of the topic.\n",
    "    Return ONLY a JSON array of strings, nothing else.\n",
    "    \n",
    "    Example format:\n",
    "    [\"query 1\", \"query 2\", \"query 3\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=planner_prompt)])\n",
    "    \n",
    "    try:\n",
    "        # Try to parse as JSON\n",
    "        search_queries = json.loads(response.content)\n",
    "    except:\n",
    "        # If parsing fails, try to extract queries from the text\n",
    "        import re\n",
    "        queries = re.findall(r'\"([^\"]+)\"', response.content)\n",
    "        if not queries:\n",
    "            # Fallback: split by lines or common delimiters\n",
    "            queries = [q.strip('\"') for q in response.content.split('\\n') if q.strip()]\n",
    "        search_queries = queries[:5]  # Limit to 5 queries\n",
    "    \n",
    "    print(f\"Generated queries: {search_queries}\")\n",
    "    return {\"search_queries\": search_queries, \"current_query_index\": 0}\n",
    "\n",
    "# Process Query with Agentic RAG: Use the Agentic RAG workflow for each sub-query\n",
    "def process_query_with_rag(state: ResearchState):\n",
    "    \"\"\"\n",
    "    Processes a single query using the Agentic RAG workflow.\n",
    "    \"\"\"\n",
    "    print(\"---PROCESSING QUERY WITH AGENTIC RAG---\")\n",
    "    current_index = state[\"current_query_index\"]\n",
    "    current_query = state[\"search_queries\"][current_index]\n",
    "    \n",
    "    print(f\"Processing query {current_index + 1}/{len(state['search_queries'])}: {current_query}\")\n",
    "    \n",
    "    # Initialize the Agentic RAG state for this query\n",
    "    initial_rag_state = {\"messages\": [HumanMessage(content=current_query)], \"rag_retries\": 0}\n",
    "    \n",
    "    # Run the Agentic RAG workflow\n",
    "    final_rag_state = None\n",
    "    for event in rag_graph.stream(initial_rag_state, {\"recursion_limit\": 25}):\n",
    "        # Get the final state\n",
    "        final_rag_state = event\n",
    "    \n",
    "    # Extract the answer\n",
    "    answer = final_rag_state[\"generate_answer\"][\"messages\"][0].content\n",
    "    \n",
    "    # Update the query_answers dictionary\n",
    "    query_answers = state.get(\"query_answers\", {}).copy()\n",
    "    query_answers[current_query] = answer\n",
    "    \n",
    "    print(f\"Answer for '{current_query}': {answer}\")\n",
    "    \n",
    "    return {\"query_answers\": query_answers}\n",
    "\n",
    "# Move to Next Query: Advance to the next query or finish\n",
    "def move_to_next_query(state: ResearchState):\n",
    "    \"\"\"\n",
    "    Advances to the next query or moves to report generation.\n",
    "    \"\"\"\n",
    "    print(\"---MOVING TO NEXT QUERY---\")\n",
    "    current_index = state[\"current_query_index\"]\n",
    "    total_queries = len(state[\"search_queries\"])\n",
    "    \n",
    "    if current_index + 1 < total_queries:\n",
    "        print(f\"Moving from query {current_index + 1} to {current_index + 2}\")\n",
    "        return {\"current_query_index\": current_index + 1}\n",
    "    else:\n",
    "        print(\"All queries processed, moving to final report generation\")\n",
    "        return {\"current_query_index\": current_index + 1}  # This will exceed the list length\n",
    "\n",
    "# Final Report Generator: Create comprehensive report from all answers\n",
    "def generate_final_report(state: ResearchState):\n",
    "    \"\"\"\n",
    "    Generates a final comprehensive report based on all query answers.\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING FINAL RESEARCH REPORT---\")\n",
    "    original_query = state[\"original_query\"]\n",
    "    query_answers = state[\"query_answers\"]\n",
    "    \n",
    "    # Check if we have any answers\n",
    "    if not query_answers:\n",
    "        final_report = f\"# Research Report: {original_query}\\n\\nNo relevant information could be found for this topic.\"\n",
    "        return {\"final_report\": final_report}\n",
    "    \n",
    "    # Format the query answers for the report\n",
    "    formatted_answers = \"\\n\\n\".join([f\"## {query}\\n\\n{answer}\" for query, answer in query_answers.items()])\n",
    "    \n",
    "    report_prompt = f\"\"\"You are a research report writer. Create a comprehensive report on the topic: \"{original_query}\".\n",
    "    \n",
    "    Use the following research findings to create your report:\n",
    "    \n",
    "{formatted_answers}\n",
    "    \n",
    "    Please structure your report with:\n",
    "    1. A title with the original query\n",
    "    2. An introduction explaining the importance of the topic\n",
    "    3. Detailed sections for each research query with key findings (use the query as a subheading)\n",
    "    4. A conclusion summarizing the overall findings and their significance\n",
    "    5. Proper formatting with markdown headers\n",
    "    6. Keep the report professional and well-organized\n",
    "    \n",
    "    Report:\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=report_prompt)])\n",
    "    final_report = response.content\n",
    "    \n",
    "    print(\"Final research report generated successfully.\")\n",
    "    return {\"final_report\": final_report}\n",
    "\n",
    "# Check if all queries are processed\n",
    "def are_all_queries_processed(state: ResearchState):\n",
    "    \"\"\"\n",
    "    Determines whether all queries have been processed.\n",
    "    \"\"\"\n",
    "    current_index = state[\"current_query_index\"]\n",
    "    total_queries = len(state[\"search_queries\"])\n",
    "    \n",
    "    if current_index < total_queries:\n",
    "        return \"process_query\"\n",
    "    else:\n",
    "        return \"generate_report\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deep_research_graph",
   "metadata": {},
   "source": [
    "### 5.2 Deep Research Graph Construction\n",
    "\n",
    "Build the main LangGraph workflow for the deep research system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deep_research_graph_construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LangGraph workflow for Deep Research\n",
    "research_workflow = StateGraph(ResearchState)\n",
    "\n",
    "# Add all nodes to the workflow\n",
    "research_workflow.add_node(\"search_planner\", search_planner)\n",
    "research_workflow.add_node(\"process_query_with_rag\", process_query_with_rag)\n",
    "research_workflow.add_node(\"move_to_next_query\", move_to_next_query)\n",
    "research_workflow.add_node(\"generate_final_report\", generate_final_report)\n",
    "\n",
    "# Set entry point for the workflow\n",
    "research_workflow.set_entry_point(\"search_planner\")\n",
    "\n",
    "# Add edges\n",
    "research_workflow.add_edge(\"search_planner\", \"process_query_with_rag\")\n",
    "research_workflow.add_edge(\"process_query_with_rag\", \"move_to_next_query\")\n",
    "research_workflow.add_conditional_edges(\n",
    "    \"move_to_next_query\",\n",
    "    are_all_queries_processed,\n",
    "    {\n",
    "        \"process_query\": \"process_query_with_rag\",\n",
    "        \"generate_report\": \"generate_final_report\"\n",
    "    }\n",
    ")\n",
    "research_workflow.add_edge(\"generate_final_report\", END)\n",
    "\n",
    "# Compile the graph into an executable workflow\n",
    "research_graph = research_workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing_validation",
   "metadata": {},
   "source": [
    "## 6. Testing and Validation\n",
    "\n",
    "### 6.1 System Testing\n",
    "\n",
    "Test the system with a complex research query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8a326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting deep research on: What are the latest advancements in quantum computing?\n",
      "==================================================\n",
      "---GENERATING SEARCH QUERIES---\n",
      "Generated queries: ['latest quantum computing hardware innovations', 'new qubit technologies in 2023.', 'recent breakthroughs in quantum algorithms for optimization problems', 'advancements in quantum machine learning algorithms.', 'progress in quantum error correction techniques']\n",
      "---PROCESSING QUERY WITH AGENTIC RAG---\n",
      "Processing query 1/5: latest quantum computing hardware innovations\n",
      "---ROUTING QUERY---\n",
      "Routing decision: [{'name': 'web_search_tool', 'args': {'query': 'latest quantum computing hardware innovations'}, 'id': 'call_aar5m43i', 'type': 'tool_call'}]\n",
      "---PERFORMING WEB SEARCH---\n",
      "---GENERATING FINAL ANSWER---\n",
      "Answer for 'latest quantum computing hardware innovations': <think>\n",
      "Okay, the user is asking about the latest quantum computing hardware innovations. Let me check the provided context.\n",
      "\n",
      "First, the context has three entries. The first one from BlueQubit talks about innovations shaping the field but doesn't specify recent hardware. The second mentions germanium and GeSn-based quantum well lasers as materials under development. The third one from Spinquanta states that in April 2025, Fujitsu and RIKEN announced a 256-qubit superconducting quantum computer, which is four times larger than their 2023 model. \n",
      "\n",
      "So the latest hardware mentioned is the 256-qubit system from Fujitsu and RIKEN. Also, the materials like germanium are part of the innovations. I need to mention both the hardware milestone and the material developments. Keep it concise, three sentences max. Make sure to note the 2025 date for the Fujitsu announcement and the materials. Avoid any markdown, just plain text. Let me structure that.\n",
      "</think>\n",
      "\n",
      "Fujitsu and RIKEN announced a 256-qubit superconducting quantum computer in April 2025, four times larger than their 2023 model. Innovative materials like germanium and GeSn-based quantum well lasers are being developed to enhance quantum hardware performance. These advancements highlight progress in scalability and component efficiency.\n",
      "---MOVING TO NEXT QUERY---\n",
      "Moving from query 1 to 2\n",
      "---PROCESSING QUERY WITH AGENTIC RAG---\n",
      "Processing query 2/5: new qubit technologies in 2023.\n",
      "---ROUTING QUERY---\n",
      "Routing decision: [{'name': 'web_search_tool', 'args': {'query': 'new qubit technologies in 2023'}, 'id': 'call_5nds9kvq', 'type': 'tool_call'}]\n",
      "---PERFORMING WEB SEARCH---\n",
      "---GENERATING FINAL ANSWER---\n",
      "Answer for 'new qubit technologies in 2023.': <think>\n",
      "Okay, the user is asking about new qubit technologies in 2023. Let me check the provided context.\n",
      "\n",
      "First, the context mentions IBM's Condor with 1000 qubits and a 50% increase in qubit density. That's a significant advancement. Then there's MIT's new superconducting qubit circuit for higher accuracy. Also, IonQ's Forte trapped-ion quantum computer is noted. These are three key points. I need to condense this into three sentences. Make sure each sentence highlights a different technology. Avoid any extra details. Check the links to confirm they're from 2023. Yep, all the snippets are from 2023. Alright, structure the answer clearly.\n",
      "</think>\n",
      "\n",
      "In 2023, IBM introduced the Condor quantum computer with 1000 qubits and improved qubit density. MIT developed a superconducting qubit architecture for higher operation accuracy. IonQ showcased the Forte trapped-ion quantum computer, advancing error correction and scalability.\n",
      "---MOVING TO NEXT QUERY---\n",
      "Moving from query 2 to 3\n",
      "---PROCESSING QUERY WITH AGENTIC RAG---\n",
      "Processing query 3/5: recent breakthroughs in quantum algorithms for optimization problems\n",
      "---ROUTING QUERY---\n",
      "Routing decision: [{'name': 'web_search_tool', 'args': {'query': 'recent breakthroughs in quantum algorithms for optimization problems'}, 'id': 'call_uoc127co', 'type': 'tool_call'}]\n",
      "---PERFORMING WEB SEARCH---\n",
      "---GENERATING FINAL ANSWER---\n",
      "Answer for 'recent breakthroughs in quantum algorithms for optimization problems': <think>\n",
      "Okay, the user is asking about recent breakthroughs in quantum algorithms for optimization problems. Let me check the provided context.\n",
      "\n",
      "First context mentions a Chinese team's quantum search algorithm that extends Grover's speedup to continuous optimization and spectral problems. They proved quantum speedup for infinite solution spaces and established a lower bound for optimality. That's a solid point.\n",
      "\n",
      "Second context from WIRED talks about a new quantum algorithm that's faster than classical ones for a wide class of optimization problems. It's a significant speedup, which is important for the user's question.\n",
      "\n",
      "Third context mentions IBM and Kipu's hybrid algorithms combining traditional math with quantum mechanics, outperforming classical methods. Hybrid approaches are a key trend, so that's relevant.\n",
      "\n",
      "I need to condense these into three concise sentences. Start with the Chinese team's work on continuous optimization, then mention the WIRED algorithm's speedup, and finally the IBM-Kipu hybrid approach. Make sure each sentence is a separate breakthrough. Avoid technical jargon but include key terms like Grover's speedup, hybrid algorithms, and speedup over classical methods. Check the word count and ensure it's under three sentences. Alright, that should cover the main points without being too verbose.\n",
      "</think>\n",
      "\n",
      "Recent breakthroughs include a quantum algorithm from China extending Grover's speedup to continuous optimization with proven efficiency for infinite solution spaces. A WIRED report highlights a quantum method outperforming classical algorithms for a broad class of hard optimization problems. IBM and Kipu's hybrid quantum-classical approach also demonstrated superior performance in complex optimization tasks.\n",
      "---MOVING TO NEXT QUERY---\n",
      "Moving from query 3 to 4\n",
      "---PROCESSING QUERY WITH AGENTIC RAG---\n",
      "Processing query 4/5: advancements in quantum machine learning algorithms.\n",
      "---ROUTING QUERY---\n",
      "Routing decision: [{'name': 'web_search_tool', 'args': {'query': 'advancements in quantum machine learning algorithms'}, 'id': 'call_zzlqxv1j', 'type': 'tool_call'}]\n",
      "---PERFORMING WEB SEARCH---\n",
      "---GENERATING FINAL ANSWER---\n",
      "Answer for 'advancements in quantum machine learning algorithms.': <think>\n",
      "Okay, the user is asking about advancements in quantum machine learning algorithms. Let me check the provided context.\n",
      "\n",
      "The first context mentions a new algorithm that adapts classical ML techniques for quantum computers. The second one from 2023 talks about a thorough analysis of QML and QDL algorithms, their benefits, and applications. The third snippet emphasizes the need for specialized quantum algorithms to leverage quantum advantages.\n",
      "\n",
      "So, the key points are: adapting classical methods, thorough analysis of QML/QDL with benefits, and the necessity of specialized algorithms. I need to condense this into three sentences. Make sure to mention the algorithm development, the analysis from the 2023 paper, and the importance of tailored algorithms. Keep it concise and within the sentence limit.\n",
      "</think>\n",
      "\n",
      "Advancements in quantum machine learning include new algorithms adapting classical techniques for quantum computers. Recent analyses highlight benefits like enhanced efficiency and novel applications in quantum deep learning. Specialized quantum algorithms are critical to fully harness computational advantages.\n",
      "---MOVING TO NEXT QUERY---\n",
      "Moving from query 4 to 5\n",
      "---PROCESSING QUERY WITH AGENTIC RAG---\n",
      "Processing query 5/5: progress in quantum error correction techniques\n",
      "---ROUTING QUERY---\n",
      "Routing decision: [{'name': 'web_search_tool', 'args': {'query': 'progress in quantum error correction techniques'}, 'id': 'call_c45poqsv', 'type': 'tool_call'}]\n",
      "---PERFORMING WEB SEARCH---\n",
      "---GENERATING FINAL ANSWER---\n",
      "Answer for 'progress in quantum error correction techniques': <think>\n",
      "Okay, the user is asking about progress in quantum error correction techniques. Let me look at the context provided.\n",
      "\n",
      "First, the Google blog mentions combining physical qubits into logical ones to correct errors faster than they accumulate. That's a key point. Then, the Physics World article talks about Lukin and Bluvstein's 48 logical qubit processor that corrects errors in real time. IBM's blog says they're making progress but current hardware is using error mitigation. \n",
      "\n",
      "I need to summarize these in three sentences. Start with the combination of qubits from Google. Mention the real-time correction from the second source. Finally, note IBM's progress and hardware utilization. Keep it concise and under three sentences. Make sure each sentence is a separate point without extra details.\n",
      "</think>\n",
      "\n",
      "Recent advancements in quantum error correction (QEC) include combining multiple physical qubits into robust logical qubits, enabling faster error correction than error accumulation (Google). A 48-logical-qubit processor now executes algorithms with real-time error correction, showcasing scalable QEC implementation (Physics World). IBM highlights ongoing progress in error correction techniques, though current hardware relies heavily on error mitigation strategies.\n",
      "---MOVING TO NEXT QUERY---\n",
      "All queries processed, moving to final report generation\n",
      "---GENERATING FINAL RESEARCH REPORT---\n",
      "Final research report generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "question = \"What are the latest advancements in quantum computing?\"\n",
    "\n",
    "# Initialize state for the research workflow\n",
    "initial_research_state = {\n",
    "    \"messages\": [HumanMessage(content=question)],\n",
    "    \"original_query\": question,\n",
    "    \"search_queries\": [],\n",
    "    \"query_answers\": {},\n",
    "    \"current_query_index\": 0,\n",
    "    \"final_report\": \"\"\n",
    "}\n",
    "\n",
    "# Uncomment the following lines to run the test\n",
    "print(f\"Starting deep research on: {question}\")\n",
    "print(\"=\" * 50)\n",
    "final_state = list(research_graph.stream(initial_research_state, {\"recursion_limit\": 100}))\n",
    "print(final_state[-1]['generate_final_report']['final_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowledge_base_management",
   "metadata": {},
   "source": [
    "### 6.2 Knowledge Base Management\n",
    "\n",
    "Code to rebuild the FAISS vector database from source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db74bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Source URLs for LLM-related content\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load documents from URLs\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "print(f\"len of documents :{len(docs_list)}\")\n",
    "\n",
    "# Split documents into chunks for better retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=512, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "print(f\"length of document chunks generated :{len(doc_splits)}\")\n",
    "\n",
    "# Create and save FAISS vector database\n",
    "db_path = './db/faiss_ilianweng_db'\n",
    "vector_db = FAISS.from_documents(doc_splits, HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\"),)\n",
    "        \n",
    "# Save database to disk\n",
    "os.makedirs(os.path.split(db_path)[0], exist_ok=True)\n",
    "vector_db.save_local(db_path)\n",
    "print(f\"\\nDatabase update complete! Total chunks: {vector_db.index.ntotal}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
